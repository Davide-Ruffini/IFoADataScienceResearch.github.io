---
title: " All clear: How Shapley values make opaque models more transparents"
author_profile: false 
classes: wide
comments: true
sidebar:
  title: "Contents"
  nav: unsupervised-nav
categories:
  - Tutorial
tags:
  - Unsupervised Learning
  - Neural Networks
---

Originally published by The Actuary, March 2022. © The Institute and Faculty of Actuaries. <br>
<a href="https://www.theactuary.com/features/2022/03/01/all-clear-how-shapley-values-make-opaque-models-more-transparent"> Click here to read the original article</a>.
{: .notice}

<b> Karol Gawlowksi, Christian Richard and Dylan Liew show how Shapley values can be used to make opaque models more transparent. </b>

<img src="/assets/images_for_posts/all-clear-shapley-values/01.png" style="width: auto; height: auto;max-width: 750px;max-height: 750px">


It is becoming evident that actuaries will need to embrace and adapt to the age of machine learning. Techniques such as gradient boosting machines (GBM) and neural networks (NN) have been shown to outperform ‘traditional’ generalised linear models (GLMs), but seem to require a trade-off in terms of the model output’s explainability. With actuaries operating under strict frameworks, it is unsurprising that many prefer to use less powerful but more explainable models.


<br>
<b> Karol Gawlowski </b> is a senior actuarial analyst at AXA XL London <br>
<b> Christian Richard </b> is a life actuary at Zurich International <br>
<b> Dylan Liew </b> is the deputy chair of the IFoA’s Explainable Artificial Intelligence Working Party and a data science actuary at Bupa
